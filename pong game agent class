# -*- coding: utf-8 -*-
"""
Created on Sat Dec 28 13:31:13 2019

@author: melorine
"""

import random
import numpy as np

from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten
from keras.layers.convolutional import Conv2D
from collections import deque

#%%
NBACTIONS = 3
IMGHEIGHT = 40
IMGWIDTH = 40 # make images smaller
IMGHISTORY = 4

OBSERVEPERIOD = 2000
GAMMA = 0.975
BATCH_SIZE = 64

ExpReplay_CAPACITY = 2000

#%%

class Agent:
    
    def __init__(self):
        self.model = self.createModel()
        self.ExpReplay = deque()
        self.steps = 0
        self.epsilon = 1.0
    
    def createModel(self):
        model = Sequential()
        
        # Convolution part
        # number of filters, kernel size, 
        model.add(Conv2D(32, kernel_size=4, strides = (2,2), input_shape = (IMGHEIGHT, IMGWIDTH, IMGHISTORY), padding = 'same'))
        model.add(Activation('relu'))
        model.add(Conv2D(64, kernel_size=4, strides = (2,2), padding = 'same'))
        model.add(Activation('relu'))
        model.add(Conv2D(64, kernel_size=3, strides = (1,1), padding = 'same'))
        model.add(Activation('relu'))
        
        # Neural network part
        model.add(Flatten())
        model.add(Dense(512))
        model.add(Activation('relu'))
        model.add(Dense(NBACTIONS, activation = 'linear')) # output layer of NN
        
        model.compile(loss = 'mse', optimizer = 'adam')
        
        return model
        
    def FindBestAct(self, s):
        
        # explore
        if random.random() < self.epsilon or self.steps < OBSERVEPERIOD:
            return random.randint(0, NBACTIONS-1) # 0, 1 or 2
        
        # exploit
        else:
            qvalue = self.model.predict(s) # our models output is Q(s,a) values
            bestA = np.argmax(qvalue)
            return bestA
    
    def CaptureSample(self, sample):
        
        self.ExpReplay.append(sample)
        if len(self.ExpReplay) > ExpReplay_CAPACITY:
            self.ExpReplay.popleft() # throw old experineces
            
        self.steps += 1
        self.epsilon = 1.0
        
        # epsilon value decreases gradually, bc I will rely on my experiences more
        if self.steps > OBSERVEPERIOD:
            self.epsilon = 0.75
            if self.steps > 7000:
                self.epsilon = 0.5
            if self.steps > 14000:
                self.epsilon = 0.25
            if self.steps > 30000:
                self.epsilon = 0.15
            if self.steps > 45000:
                self.epsilon = 0.1     
            if self.steps > 70000:
                self.epsilon = 0.05
    
    def Process(self):
        
        # whener my ExpReplay fills, start trainning
        if self.steps > OBSERVEPERIOD:
            # take random 64 samples from ExpReplay 
            minibach = random.sample(self.ExpReplay, BATCH_SIZE)
            batchlen = len(minibach)
            
            # 64 inputs, image = 40x40 pixels, 4 images  
            inputs = np.zeros(BATCH_SIZE, IMGHEIGHT, IMGWIDTH, IMGHISTORY)
            targets = np.zeros(inputs.shape[0], NBACTIONS)
            
            Q_sa = 0
            
            for i in range(batchlen):
                state_t = minibatch[i][0]
                action_t = minibatch[i][1]
                reward_t = minibatch[i][2]
                state_t1 = minibatch[i][3] # next_state
                
                inputs[i:i+1] = state_t # i th input 
                targets[i] = self.model.predict(state_t)
                
                Q_sa = self.model.predict(state_t1)
                
                if state_t1 is None:
                    targets[i, action_t] = reward_t
                    
                else:
                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa) # using bellman equation
                    
            self.model.fit(inputs, targets, batch_size = BATCH_SIZE, epochs = 1, verbose = 0)                    
            
    
    
 
